{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pablocodes1993/finalproject/blob/main/CUSTOMER_SEGMENTATION_USING_UNSUPERVISED_METHODS_AND_RFM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iVMJOTydDUm"
      },
      "source": [
        "# LIBRARY IMPORTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWqvXaWFjtzq",
        "outputId": "28c6a3ec-d1a9-49d0-e73b-79c7619cbab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting weightedstats\n",
            "  Downloading weightedstats-0.4.1-py3-none-any.whl (3.8 kB)\n"
          ]
        }
      ],
      "source": [
        "pip install weightedstats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyclustering"
      ],
      "metadata": {
        "id": "uT8wl2Ua4zIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install minisom"
      ],
      "metadata": {
        "id": "FEuiGncejsF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn-extra\n"
      ],
      "metadata": {
        "id": "LZmtod5qzEam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBdXuKubJwao"
      },
      "outputs": [],
      "source": [
        "#Library Importation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import HTML\n",
        "from datetime import timedelta\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "import matplotlib.style as style\n",
        "\n",
        "\n",
        "\n",
        "import math\n",
        "from statistics import median\n",
        "from scipy.stats import skew\n",
        "import weightedstats as ws\n",
        "from statsmodels.stats.stattools import medcouple\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from pyclustering.cluster.kmedoids import kmedoids\n",
        "from pyclustering.utils.metric import distance_metric, type_metric\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from minisom import MiniSom\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0BHZMwTc4uQ"
      },
      "source": [
        "# DATA IMPORTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9lLj4iFVr2T"
      },
      "outputs": [],
      "source": [
        "# Datasets importation from Google Drive folder\n",
        "#Customer Dataset\n",
        "customers_url = 'https://drive.google.com/file/d/1trjIdykod5jlh4JCk7ZRSoKXFvovra68/view?usp=share_link'\n",
        "customers_path = 'https://drive.google.com/uc?export=download&id='+customers_url.split('/')[-2]\n",
        "customers_data = pd.read_csv(customers_path)\n",
        "#Geolocation Dataset\n",
        "geolocation_url = 'https://drive.google.com/file/d/1g635f6qGuFpqtIKe63ss2X6YgOHSLDLR/view?usp=share_link'\n",
        "geolocation_path = 'https://drive.google.com/uc?export=download&id='+geolocation_url.split('/')[-2]\n",
        "geolocation_data = pd.read_csv(geolocation_path)\n",
        "#Items Dataset\n",
        "items_url = 'https://drive.google.com/file/d/18acVf2ciN0As5Q5GbA2PnO6HyF_dOrz4/view?usp=share_link'\n",
        "items_path = 'https://drive.google.com/uc?export=download&id='+items_url.split('/')[-2]\n",
        "items_data = pd.read_csv(items_path)\n",
        "#Payments Dataset\n",
        "payments_url = 'https://drive.google.com/file/d/1CH48eJjls-_gfjMPXAAxtx1Sh0cGUq67/view?usp=share_link'\n",
        "payments_path = 'https://drive.google.com/uc?export=download&id='+payments_url.split('/')[-2]\n",
        "payments_data = pd.read_csv(payments_path)\n",
        "#Reviews Dataset\n",
        "reviews_url = 'https://drive.google.com/file/d/14EEawWIHYafD-kLbJUVeYIM9L-c1nw24/view?usp=share_link'\n",
        "reviews_path = 'https://drive.google.com/uc?export=download&id='+reviews_url.split('/')[-2]\n",
        "reviews_data = pd.read_csv(reviews_path)\n",
        "# Orders Dataset\n",
        "orders_url = 'https://drive.google.com/file/d/1QVPJERDEvBFq6tLPEvfH-OfhdHOWQV67/view?usp=share_link'\n",
        "orders_path = 'https://drive.google.com/uc?export=download&id='+orders_url.split('/')[-2]\n",
        "orders_data = pd.read_csv(orders_path)\n",
        "\n",
        "# Products Dataset\n",
        "products_url = 'https://drive.google.com/file/d/1HlLVfY8GrjIxRXsvqm8FaIdJS4WIIL1Q/view?usp=share_link'\n",
        "products_path = 'https://drive.google.com/uc?export=download&id='+products_url.split('/')[-2]\n",
        "products_data = pd.read_csv(products_path)\n",
        "# Sellers Dataset\n",
        "sellers_url = 'https://drive.google.com/file/d/18s2z6P6noAyaRSK2ttTsU_g3porLucOg/view?usp=share_link'\n",
        "sellers_path = 'https://drive.google.com/uc?export=download&id='+sellers_url.split('/')[-2]\n",
        "sellers_data= pd.read_csv(sellers_path)\n",
        "# Sellers Dataset\n",
        "category_name_trans_url= 'https://drive.google.com/file/d/12ApARlTizxPK3M9pAmcVbpMhQm5zPuf2/view?usp=share_link'\n",
        "category_name_trans_path = 'https://drive.google.com/uc?export=download&id='+category_name_trans_url.split('/')[-2]\n",
        "category_name_trans_data= pd.read_csv(category_name_trans_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXhtDtgZP65_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Datasets names\n",
        "datasets = [\n",
        "    customers_data,\n",
        "    geolocation_data,\n",
        "    items_data,\n",
        "    payments_data,\n",
        "    reviews_data,\n",
        "    orders_data,\n",
        "    products_data,\n",
        "    sellers_data,\n",
        "    category_name_trans_data\n",
        "]\n",
        "titles = [\n",
        "    \"Customers\",\n",
        "    \"Geolocation\",\n",
        "    \"Items\",\n",
        "    \"Payments\",\n",
        "    \"Reviews\",\n",
        "    \"Orders\",\n",
        "    \"Products\",\n",
        "    \"Sellers\",\n",
        "    \"Category Translation\"\n",
        "]\n",
        "\n",
        "# Store the data\n",
        "data_summary = {\n",
        "    'Datasets': titles,\n",
        "    'Columns': [', '.join(data.columns) for data in datasets],\n",
        "    'Total Rows': [data.shape[0] for data in datasets],\n",
        "    'Total Columns': [data.shape[1] for data in datasets],\n",
        "    'Total Duplicates': [data.duplicated().sum() for data in datasets],\n",
        "    'Total Null': [data.isnull().sum().sum() for data in datasets],\n",
        "    'Null Columns': [', '.join(data.columns[data.isnull().any()]) for data in datasets]\n",
        "}\n",
        "\n",
        "#  Appended Dataframe for analysis\n",
        "data_summary_df = pd.DataFrame(data_summary)\n",
        "\n",
        "# Color gradient\n",
        "data_summary_styled = data_summary_df.style.background_gradient(cmap='YlGnBu')\n",
        "\n",
        "# Display the styled DataFrame\n",
        "data_summary_styled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mhQ4LZ34e9P"
      },
      "outputs": [],
      "source": [
        "orders_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKFauDL84wx6"
      },
      "outputs": [],
      "source": [
        "# Count of missing Values\n",
        "print(\" \\nCount of missing values : \\n\\n\",\n",
        "      orders_data.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO9bLN4gjdNM"
      },
      "source": [
        "# MERGING THE DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8t7t91Ma0Rj8"
      },
      "outputs": [],
      "source": [
        "#Datasets merging process with key variables\n",
        "olist_dataset= pd.merge(customers_data, orders_data, on=\"customer_id\", how='inner')\n",
        "olist_dataset= olist_dataset.merge(reviews_data, on=\"order_id\", how='inner')\n",
        "olist_dataset= olist_dataset.merge(items_data, on=\"order_id\", how='inner')\n",
        "olist_dataset= olist_dataset.merge(products_data, on=\"product_id\", how='inner')\n",
        "olist_dataset= olist_dataset.merge(payments_data, on=\"order_id\", how='inner')\n",
        "olist_dataset= olist_dataset.merge(sellers_data, on='seller_id', how='inner')\n",
        "olist_dataset= olist_dataset.merge(category_name_trans_data, on='product_category_name', how='inner')\n",
        "\n",
        "olist_dataset.shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xANcw47XRd7v"
      },
      "source": [
        "Column names of Olist merged dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovub49iU1due"
      },
      "outputs": [],
      "source": [
        "olist_dataset.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9nKxLu61oGM"
      },
      "outputs": [],
      "source": [
        "olist_dataset.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amEDJpro19x8"
      },
      "outputs": [],
      "source": [
        "olist_dataset.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpErsa7NR3pG"
      },
      "source": [
        "# **DATA PREPROCESSING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZB8ma6SSsvE"
      },
      "outputs": [],
      "source": [
        "#Store the Pre processed data in a new dataframe\n",
        "olist_df=olist_dataset.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqUj89ACR9BC"
      },
      "source": [
        "## REMOVAL OF USELESS COLUMNS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHeiCOzFS2K3"
      },
      "outputs": [],
      "source": [
        "olist_df.drop(['review_comment_title', 'review_comment_message', 'review_creation_date', 'review_answer_timestamp', 'product_name_lenght',\n",
        "                'review_id', 'product_photos_qty', 'payment_sequential', 'payment_installments' ], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ7M5okVSa-2"
      },
      "source": [
        "## MISSING VALUES TREATMENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xV4JJ07GSFb6"
      },
      "outputs": [],
      "source": [
        "# Percentage of missing data\n",
        "missing_data = pd.concat([(olist_df.isnull().sum()), (olist_df.isnull().sum() * 100 / olist_df.isnull().count())], axis=1, keys=['Total', 'Percentage of Missing Values']).sort_values(by='Total', ascending=False)\n",
        "missing_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn9W_8xYT5rN"
      },
      "outputs": [],
      "source": [
        "# Calculate the count and percentage of each order status\n",
        "order_status_counts = olist_df['order_status'].value_counts()\n",
        "order_status_percentages = olist_df['order_status'].value_counts(normalize=True) * 100\n",
        "\n",
        "# Create a DataFrame to store the count and percentage information\n",
        "order_status_summary = pd.DataFrame({\n",
        "    'Order Status Count': order_status_counts,\n",
        "    'Order Status Percentage': order_status_percentages.round(2)\n",
        "})\n",
        "\n",
        "# Display the order status summary\n",
        "order_status_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SxuMpUStVCy"
      },
      "outputs": [],
      "source": [
        "# Order status of orders where: order delivered carrier date and order deliveres customer date are null\n",
        "selected_rows = olist_df.loc[olist_df['order_delivered_carrier_date'].isna() & olist_df['order_delivered_customer_date'].isna()]\n",
        "\n",
        "status_counts = selected_rows['order_status'].value_counts()\n",
        "\n",
        "print(status_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVaS2AJLWdCk"
      },
      "source": [
        "All the orders that have order_status equal to CANCELED will be removed. These columns do not provide any information for further analysis in customer segmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DP-SJsrNT7Ox"
      },
      "outputs": [],
      "source": [
        "# Drop all the orders that have missing delivered date and are canceled\n",
        "#olist_df = olist_df.drop(olist_df.loc[(olist_df['order_status'] == 'canceled')].index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJ_1uIj-USRr"
      },
      "outputs": [],
      "source": [
        "# Count of missing Values\n",
        "print(\" \\nCount of missing values : \\n\\n\",\n",
        "      olist_df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm4F3bW1ViXB"
      },
      "source": [
        "There are 14 orders that have missing values in order_approved_at column but have a status of DELIVERED. Since these rows where delivered, the columns will not be removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XnCgdSEUeXP"
      },
      "outputs": [],
      "source": [
        "#Missing values in order delivered carrier date\n",
        "olist_df.loc[olist_df['order_approved_at'].isna(),\n",
        "                   ['order_purchase_timestamp', 'order_delivered_carrier_date', 'order_delivered_customer_date','order_status']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWP5m3_KXxyW"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Fill all the Nan rows from order_approved_at column with Nat(no time)\n",
        "olist_df['order_approved_at']= olist_df['order_approved_at'].fillna(pd.NaT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gabI-w08W8eb"
      },
      "source": [
        "There are 725 orders that have nan values in order_delivered_customer_date and order_delivered_carrier_date and the order status is not delivered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60n7_euxVMhS"
      },
      "outputs": [],
      "source": [
        "# Order status of orders where: order delivered carrier date and order deliveres customer date are null\n",
        "selected_rows = olist_df.loc[olist_df['order_delivered_carrier_date'].isna() & olist_df['order_delivered_customer_date'].isna()]\n",
        "\n",
        "status_counts = selected_rows['order_status'].value_counts()\n",
        "\n",
        "print(status_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwqruBFAb3eO"
      },
      "outputs": [],
      "source": [
        "# Drop all the orders that have nan values in order_delivered_customer_date and order_delivered_carrier_date\n",
        "#olist_df= olist_df.drop(olist_df.loc[(olist_df['order_delivered_customer_date'].isna()) & (olist_df['order_delivered_carrier_date'].isna())\n",
        " #          ].index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWkcQALqb_Ss"
      },
      "outputs": [],
      "source": [
        "#Missing values in APPROVED orders that  were delivered\n",
        "olist_df.loc[  olist_df['order_delivered_customer_date'].isna() ,\n",
        "                   ['order_purchase_timestamp','order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date','order_status', 'review_score' ]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voK-htAlcQ-7"
      },
      "outputs": [],
      "source": [
        "# Order status of orders where: order delivered carrier date and order deliveres customer date are null\n",
        "selected_rows2 = olist_df.loc[olist_df['order_delivered_customer_date'].isna()]\n",
        "\n",
        "status_counts2 = selected_rows2['order_status'].value_counts()\n",
        "\n",
        "print(status_counts2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAS6IUEre3Wm"
      },
      "outputs": [],
      "source": [
        "olist_df['payment_type'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEVykVbbfhyh"
      },
      "source": [
        "There is one product of the dataset that does not contain information about its size. Therefore it will be removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3R6qoICyfVF6"
      },
      "outputs": [],
      "source": [
        "#Product that has no volume\n",
        "null_mask = olist_df['product_length_cm'].isnull()\n",
        "null_rows = olist_df[null_mask]\n",
        "null_rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wv0GDiPPf2MH"
      },
      "outputs": [],
      "source": [
        "#Drop the selected product without information\n",
        "null_mask = olist_df['product_length_cm'].isnull()\n",
        "null_rows = olist_df[null_mask]\n",
        "olist_df = olist_df.drop(null_rows.index).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiZwtpAHgwug"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyJloMZQXJuR"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAVEYRcvW7dG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iGcwtfxW33Z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj_2ukOamGq6"
      },
      "source": [
        "#DATA UNDERSTANDING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_v2QtAHhZSL"
      },
      "outputs": [],
      "source": [
        "#Store the Pre processed data in a new dataframe\n",
        "olist_df_cleaned=olist_df.copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8SC7VuMhilL"
      },
      "outputs": [],
      "source": [
        "\n",
        "olist_df_cleaned[['order_status','customer_city','customer_state']].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8JfQwnOjhK2"
      },
      "source": [
        "# RFM ANALYSIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t3MDJgFlcmi"
      },
      "source": [
        "\n",
        "\n",
        "*   Recency:\n",
        "*   Frequency: number of order\n",
        "*   Monetary:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2hv4E7ljLk7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Convert 'order_purchase_timestamp' to datetime type\n",
        "olist_df_cleaned['order_purchase_timestamp'] = pd.to_datetime(olist_df_cleaned['order_purchase_timestamp'])\n",
        "\n",
        "# Set the date for analysis\n",
        "current_date = olist_df_cleaned['order_purchase_timestamp'].max() + pd.DateOffset(days=1)\n",
        "print(current_date)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFVgBGiGjLim"
      },
      "outputs": [],
      "source": [
        "#Recency calculation df\n",
        "sorted_recency_df = olist_df_cleaned.sort_values('order_purchase_timestamp', ascending=False)\n",
        "recency_df = sorted_recency_df.loc[:, ['order_purchase_timestamp', 'customer_unique_id']]\n",
        "recency_df = olist_df_cleaned.groupby('customer_unique_id').apply(lambda x: pd.Series({\n",
        "   'order_purchase_timestamp': x['order_purchase_timestamp'].max(),\n",
        "    'Recency': (current_date - x['order_purchase_timestamp'].max()).days\n",
        "\n",
        "})).reset_index()\n",
        "\n",
        "#Sort the recency column\n",
        "#recency_df  = recency_df.sort_values('Recency', ascending=False)\n",
        "\n",
        "\n",
        "recency_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItB7f7lkjLfo"
      },
      "outputs": [],
      "source": [
        "#Frequency calculation df\n",
        "frequency_df2 = pd.DataFrame( olist_df_cleaned.groupby([\"customer_unique_id\"]).agg({\"order_id\":\"nunique\"}).reset_index())\n",
        "frequency_df2.rename(columns={\"order_id\":\"Frequency\"}, inplace=True)\n",
        "#frequency_df\n",
        "\n",
        "frequency_df=olist_df_cleaned.groupby('customer_unique_id')['product_id'].size().reset_index(name='Frequency')\n",
        "#Sort the recency column\n",
        "frequency_df  = frequency_df.sort_values('Frequency', ascending=False)\n",
        "\n",
        "frequency_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryrONsc4jLdd"
      },
      "outputs": [],
      "source": [
        "#Monetary Calculation df\n",
        "monetary_df=olist_df_cleaned.groupby('customer_unique_id')['payment_value'].sum().reset_index(name='Monetary')\n",
        "#Sort the recency column\n",
        "monetary_df  = monetary_df.sort_values('Monetary', ascending=False)\n",
        "\n",
        "monetary_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lAD4T-mjLaW"
      },
      "outputs": [],
      "source": [
        "monetary_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uL9KmDjhjLX5"
      },
      "outputs": [],
      "source": [
        "#Datasets merging process with key variables\n",
        "RFM_df= pd.merge(recency_df, frequency_df2, on=\"customer_unique_id\", how='inner')\n",
        "RFM_df= RFM_df.merge(monetary_df, on=\"customer_unique_id\", how='inner')\n",
        "RFM_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORDuRarjjLVV"
      },
      "outputs": [],
      "source": [
        "# Compute quantiles for each feature\n",
        "quantiles = RFM_df.quantile(q=[0.2, 0.4, 0.6, 0.8])\n",
        "\n",
        "# Assign quartile labels for each feature using apply() method\n",
        "RFM_Segment = RFM_df.copy()\n",
        "RFM_Segment['R_Quartile'] = RFM_Segment['Recency'].apply(lambda x: sum(x <= quantiles['Recency'].values) + 1)\n",
        "RFM_Segment['F_Quartile'] = RFM_Segment['Frequency'].apply(lambda x: sum(x <= quantiles['Frequency'].values) + 1)\n",
        "RFM_Segment['M_Quartile'] = RFM_Segment['Monetary'].apply(lambda x: sum(x >= quantiles['Monetary'].values) + 1)\n",
        "\n",
        "# Concatenate the quartile labels to form the RFMClass label\n",
        "RFM_Segment['RFMClass'] = RFM_Segment['R_Quartile'].map(str) \\\n",
        "                            + RFM_Segment['F_Quartile'].map(str) \\\n",
        "                            + RFM_Segment['M_Quartile'].map(str)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "380mXRDRxSuZ"
      },
      "outputs": [],
      "source": [
        "quantiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUuv-azlnyil"
      },
      "outputs": [],
      "source": [
        "RFM_Segment['RFMClass'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVylfRoFyGHv"
      },
      "outputs": [],
      "source": [
        "RFM_Segment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoUWlJKVgO8r"
      },
      "outputs": [],
      "source": [
        "RFM_Segment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3yQxSC_gjeJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Merge the RFM class to the RFM_df\n",
        "RFM_df = pd.merge(RFM_df, RFM_Segment[['customer_unique_id', 'RFMClass']], on='customer_unique_id', how='left')\n",
        "\n",
        "# Print the merged DataFrame\n",
        "print(RFM_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dT0Nr5Rnn341"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have an RFM DataFrame named 'RFM_df' with columns 'Recency', 'Frequency', 'Monetary'\n",
        "# Replace 'RFM_df' with your actual DataFrame containing RFM data\n",
        "rfm_data = RFM_df\n",
        "\n",
        "# Perform univariate analysis for each RFM metric\n",
        "for metric in ['Recency', 'Frequency', 'Monetary']:\n",
        "    # Calculate descriptive statistics\n",
        "    statistics = rfm_data[metric].describe()\n",
        "    print(f\"Descriptive statistics for {metric}:\\n{statistics}\\n\")\n",
        "\n",
        "    # Create a histogram with distribution line\n",
        "    plt.figure()\n",
        "    sns.histplot(data=rfm_data, x=metric, bins=40, color='lightblue', edgecolor='black')\n",
        "    plt.xlabel(metric)\n",
        "    plt.ylabel('Number of Customers')\n",
        "    plt.grid(True)  # Add grid\n",
        "\n",
        "    # Calculate skewness\n",
        "    skewness = np.round(rfm_data[metric].skew(), 2)\n",
        "    print(f\"Skewness value for {metric}:\\n{skewness}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyDDaoOOn31K"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 10))\n",
        "plt.subplot(3, 1, 1); sns.distplot(RFM_df['Recency'])\n",
        "plt.subplot(3, 1, 2); sns.distplot(RFM_df['Frequency'])\n",
        "plt.subplot(3, 1, 3); sns.distplot(RFM_df['Monetary'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtgpTEu8o55o"
      },
      "outputs": [],
      "source": [
        "correlation_matrix = RFM_df[['Recency', 'Frequency', 'Monetary']].corr()\n",
        "\n",
        "\n",
        "# Plot the correlation matrix as a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GtUmHz3pd0m"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Assuming you have an RFM DataFrame named 'RFM_df' with columns 'Recency', 'Frequency', 'Monetary'\n",
        "# Replace 'RFM_df' with your actual DataFrame containing RFM data\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = RFM_df[['Recency', 'Frequency', 'Monetary']].corr()\n",
        "\n",
        "# Plot the correlogram\n",
        "sns.set(style='white')\n",
        "sns.pairplot(RFM_df[['Recency', 'Frequency', 'Monetary']], diag_kind='kde', plot_kws={'alpha': 0.6})\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_zLX_nJpiIV"
      },
      "outputs": [],
      "source": [
        "# with columns 'Recency', 'Frequency', and 'MonetaryValue'\n",
        "\n",
        "# Create a figure and axes\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
        "\n",
        "# Plot box plots for each variable\n",
        "RFM_df.boxplot(column='Recency', ax=axes[0])\n",
        "axes[0].set_title('Recency')\n",
        "axes[0].set_ylabel('Days')\n",
        "\n",
        "rfm_data.boxplot(column='Frequency', ax=axes[1])\n",
        "axes[1].set_title('Frequency')\n",
        "axes[1].set_ylabel('Count')\n",
        "\n",
        "rfm_data.boxplot(column='Monetary', ax=axes[2])\n",
        "#axes[2].set_title('Monetary')\n",
        "axes[2].set_ylabel('Amount')\n",
        "\n",
        "# Adjust the layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fX8IIf1Npn8G"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import shapiro\n",
        "\n",
        "# Assuming you have the RFM data in a pandas DataFrame called 'rfm_data'\n",
        "# with a column 'Recency'\n",
        "\n",
        "# Perform the Shapiro-Wilk test\n",
        "statistic, p_value = shapiro(rfm_data['Recency'])\n",
        "\n",
        "# Print the test results\n",
        "print('Shapiro-Wilk Test - Recency')\n",
        "print('Test Statistic:', statistic)\n",
        "print('p-value:', p_value)\n",
        "\n",
        "# Interpret the test results\n",
        "alpha = 0.05  # Set the significance level\n",
        "if p_value > alpha:\n",
        "    print('The Recency variable follows a normal distribution.')\n",
        "else:\n",
        "    print('The Recency variable does not follow a normal distribution.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qK1i6EpGpz9q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming you have an RFM DataFrame with columns 'customer_id', 'recency', 'frequency', 'monetary', and 'order_date'\n",
        "rfm_df = RFM_df\n",
        "\n",
        "# Convert 'order_date' column to datetime type\n",
        "rfm_df['order_purchase_timestamp'] = pd.to_datetime(rfm_df['order_purchase_timestamp'])\n",
        "\n",
        "# Calculate the reference date for analysis\n",
        "reference_date = rfm_df['order_purchase_timestamp'].max()\n",
        "\n",
        "# Calculate recency, frequency, and monetary values relative to the reference date\n",
        "rfm_df['Recency'] = (reference_date - rfm_df['order_purchase_timestamp']).dt.days\n",
        "rfm_df['Frequency'] = rfm_df['Frequency']\n",
        "rfm_df['Monetary'] = rfm_df['Monetary']\n",
        "\n",
        "# Perform RFM analysis over time\n",
        "rfm_over_time = rfm_df.groupby('order_purchase_timestamp').agg({\n",
        "    'Recency': 'mean',\n",
        "    'Frequency': 'mean',\n",
        "    'Monetary': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "print(rfm_over_time)\n",
        "# Convert 'order_date' column to datetime type (if not already done)\n",
        "rfm_over_time['order_purchase_timestamp'] = pd.to_datetime(rfm_over_time['order_purchase_timestamp'])\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "#plt.plot(rfm_over_time['order_purchase_timestamp'], rfm_over_time['Recency'], label='Recency')\n",
        "#plt.plot(rfm_over_time['order_purchase_timestamp'], rfm_over_time['Frequency'], label='Frequency')\n",
        "plt.plot(rfm_over_time['order_purchase_timestamp'], rfm_over_time['Monetary'], label='Monetary')\n",
        "plt.xlabel('Order Date')\n",
        "plt.ylabel('RFM Metric')\n",
        "plt.title('RFM Analysis Over Time')\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LgArQVxqC8v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_ip338YoTgH"
      },
      "source": [
        "# CLUSTER ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-z9KzC-ZM7M"
      },
      "outputs": [],
      "source": [
        "RFM_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0gypc8Zn3yt"
      },
      "outputs": [],
      "source": [
        "# Select specific columns and store them in another DataFrame\n",
        "selected_columns_rfm = RFM_df[['customer_unique_id','Recency','Frequency','Monetary']]\n",
        "selected_columns_rfm.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2bNPJTln3vo"
      },
      "outputs": [],
      "source": [
        "\n",
        "#unskew the data with data transformations\n",
        "selected_columns_rfm['Monetary']= np.log(selected_columns_rfm['Monetary'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuZJ-_9Gn3sk"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 10))\n",
        "plt.subplot(3, 1, 1); sns.distplot(selected_columns_rfm['Recency'])\n",
        "plt.subplot(3, 1, 2); sns.distplot(selected_columns_rfm['Frequency'])\n",
        "plt.subplot(3, 1, 3); sns.distplot(selected_columns_rfm['Monetary'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure and axes\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
        "\n",
        "\n",
        "\n",
        "selected_columns_rfm.boxplot(column='Monetary', ax=axes[2])\n",
        "\n",
        "axes[0].set_ylabel('Amount (log10)')\n",
        "\n",
        "# Adjust the layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9uF-N5pfiMSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sQjHj8Qs3v0"
      },
      "outputs": [],
      "source": [
        "selected_columns_rfm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCLNF-iqtKGU"
      },
      "outputs": [],
      "source": [
        "RFM_log_trans=selected_columns_rfm.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obRFM4rdtMLk"
      },
      "outputs": [],
      "source": [
        "#OUTLIERS FUNCTIONS\n",
        "def num_outlier(df_in, col_name):\n",
        "    q1 = df_in[col_name].quantile(0.25)\n",
        "    q3 = df_in[col_name].quantile(0.75)\n",
        "    iqr = q3-q1 #Interquartile range\n",
        "    fence_low  = q1-1.5*iqr\n",
        "    fence_high = q3+1.5*iqr\n",
        "    outliers_df= df_in.loc[(df_in[col_name] < fence_low) | (df_in[col_name] > fence_high)]\n",
        "    return print(\"Number of outliers in {} column: \".format(col_name), len(outliers_df)), print(\"Indexes: \", outliers_df.index)\n",
        "\n",
        "def erase_outliers(df_in, col_name):\n",
        "    q1 = df_in[col_name].quantile(0.25)\n",
        "    q3 = df_in[col_name].quantile(0.75)\n",
        "    iqr = q3 - q1  # Interquartile range\n",
        "    fence_low = q1 - 1.5 * iqr\n",
        "    fence_high = q3 + 1.5 * iqr\n",
        "\n",
        "    outliers_df = df_in.loc[(df_in[col_name] < fence_low) | (df_in[col_name] > fence_high)].copy()\n",
        "    outliers_df['Cluster'] =5\n",
        "\n",
        "    df_out = df_in.drop(outliers_df.index)\n",
        "\n",
        "    print(\"Number of outliers in {} column: {}\".format(col_name, len(outliers_df)))\n",
        "    print(\"Indexes:\", outliers_df.index)\n",
        "\n",
        "    return df_out, outliers_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10vHs6G8t__9"
      },
      "source": [
        "Outlier detection from original RFM data\n",
        "\n",
        "*   Outliers count in Transformed Monetary column: 9106\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYK6VRxNtMH2"
      },
      "outputs": [],
      "source": [
        "for i in ['Recency','Frequency',\"Monetary\"]:\n",
        "    num_outlier(RFM_df, i)\n",
        "    print(\"*\"*40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9vHN9uZtogu"
      },
      "source": [
        "Outlier detection from log transformed data\n",
        "\n",
        "\n",
        "*   Outliers count in  Transformed Monetary column: 1829\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3k0OaVrJtMEL"
      },
      "outputs": [],
      "source": [
        "for i in ['Frequency',\"Monetary\"]:\n",
        "    num_outlier(RFM_log_trans, i)\n",
        "    print(\"*\"*40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-cV2-mAtMAZ"
      },
      "outputs": [],
      "source": [
        "#Remove outliers in a separate dataframe\n",
        "# RFM_log_trans_wo will contain the input DataFrame with outliers removed\n",
        "# 'outliers' will contain the removed outliers in a separate DataFrame\n",
        "RFM_log_trans_wo, outliers = erase_outliers(RFM_log_trans, 'Monetary')\n",
        "\"\"\"for i in [\"Monetary\"]:\n",
        "    RFM_log_trans_wo=erase_outliers(RFM_log_trans, i)\n",
        "    print(\"*\"*40)\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isOETI0Ba0_M"
      },
      "outputs": [],
      "source": [
        "outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaVVN739tL8u"
      },
      "outputs": [],
      "source": [
        "RFM_log_trans_wo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2TSm8PbtL5A"
      },
      "outputs": [],
      "source": [
        "#SCALING\n",
        "\n",
        "# Assuming you have an RFM data frame called 'rfm_df' with columns 'Recency', 'Frequency', 'Monetary'\n",
        "\n",
        "# Create a copy of the RFM data frame\n",
        "RFM_log_trans_wo_scaled = RFM_log_trans_wo.copy()\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Standardize the RFM columns\n",
        "RFM_log_trans_wo_scaled[['Recency', 'Frequency', 'Monetary']] = scaler.fit_transform(RFM_log_trans_wo_scaled[['Recency', 'Frequency', 'Monetary']])\n",
        "\n",
        "# Print the standardized RFM data frame\n",
        "print(RFM_log_trans_wo_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VALIDATION FUNCTIONS\n"
      ],
      "metadata": {
        "id": "qRKCXrXbpvx9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-a1fzaXgtL1p"
      },
      "outputs": [],
      "source": [
        "#ELBOW ANALYSIS FUNCTION\n",
        "#KMEANS\n",
        "\n",
        "def elbow_method_kmeans(X, k_values):\n",
        "    distortions = []\n",
        "\n",
        "    for k in k_values:\n",
        "        kmeans = KMeans(n_clusters=k, init='random')\n",
        "        kmeans.fit(X)\n",
        "        distortions.append(kmeans.inertia_)\n",
        "\n",
        "    # Plot the elbow curve\n",
        "    plt.plot(k_values, distortions, marker='o')\n",
        "    plt.xlabel('Number of Clusters (k)')\n",
        "    plt.ylabel('WCSS')\n",
        "    plt.title('Elbow Method')\n",
        "    plt.xticks(k_values)  # Set x-axis ticks to match k_values\n",
        "    plt.show()\n",
        "\n",
        "#KMEANS++\n",
        "def elbow_method_kmeans_plusplus(X, k_values):\n",
        "    distortions = []\n",
        "\n",
        "    for k in k_values:\n",
        "        kmeans = KMeans(n_clusters=k, init='k-means++')\n",
        "        kmeans.fit(X)\n",
        "        distortions.append(kmeans.inertia_)\n",
        "\n",
        "    # Plot the elbow curve\n",
        "    plt.plot(k_values, distortions, marker='o')\n",
        "    plt.xlabel('Number of Clusters (k)')\n",
        "    plt.ylabel('WCSS')\n",
        "    plt.title('Elbow Method')\n",
        "    plt.show()\n",
        "\n",
        "#KMEDOIDS\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pyclustering.cluster.kmedoids import kmedoids\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "def elbow_method_medoids(X, k_values):\n",
        "    distortions = []\n",
        "\n",
        "    for k in k_values:\n",
        "        kmedoids_instance = kmedoids(X, initial_index_medoids=range(k), data_type='distance_matrix')\n",
        "        kmedoids_instance.process()\n",
        "        clusters = kmedoids_instance.get_clusters()\n",
        "        medoids = kmedoids_instance.get_medoids()\n",
        "        distances = pairwise_distances(X, metric='euclidean')\n",
        "        distortion = 0\n",
        "        for i in range(k):\n",
        "            cluster_points = X[clusters[i]]\n",
        "            medoid_index = medoids[i]\n",
        "            distortion += np.sum(distances[cluster_points, medoid_index])\n",
        "        distortions.append(distortion)\n",
        "\n",
        "    # Plot the elbow curve\n",
        "    plt.plot(k_values, distortions,'bo-')\n",
        "    plt.xlabel('Number of Clusters (k)')\n",
        "    plt.ylabel('Distortion')\n",
        "    plt.title('Elbow Method (K-Medoids)')\n",
        "    plt.show()\n",
        "\n",
        "#SOM\n",
        "\n",
        "\n",
        "\n",
        "#SILHOUTTE ANALYSIS FUNCTION\n",
        "#K MEANS\n",
        "def perform_rfm_analysis(X):\n",
        "    range_n_clusters = [2, 3, 4, 5, 6]\n",
        "    silhouette_avg_n_clusters = []\n",
        "\n",
        "    for n_clusters in range_n_clusters:\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "        fig.set_size_inches(18, 7)\n",
        "\n",
        "        ax1.set_xlim([-0.1, 1])\n",
        "        ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\n",
        "        clusterer = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        cluster_labels = clusterer.fit_predict(X)\n",
        "\n",
        "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "        print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)\n",
        "\n",
        "        silhouette_avg_n_clusters.append(silhouette_avg)\n",
        "        sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "        y_lower = 10\n",
        "        for i in range(n_clusters):\n",
        "            ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
        "            ith_cluster_silhouette_values.sort()\n",
        "\n",
        "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "            y_upper = y_lower + size_cluster_i\n",
        "\n",
        "            color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "            ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values,\n",
        "                              facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "            y_lower = y_upper + 10\n",
        "\n",
        "        ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "        ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "        ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "        ax1.set_yticks([])\n",
        "        ax1.set_xticks([-0.2,-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "        colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
        "        ax2.scatter(X.iloc[:, 0], X.iloc[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
        "                    c=colors, edgecolor='k')\n",
        "\n",
        "        centers = clusterer.cluster_centers_\n",
        "        ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
        "                    c=\"white\", alpha=1, s=200, edgecolor='k')\n",
        "\n",
        "        for i, c in enumerate(centers):\n",
        "            ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
        "                        s=50, edgecolor='k')\n",
        "\n",
        "        ax2.set_title(\"The visualization of the clustered data.\")\n",
        "        ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "        ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "\n",
        "        plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                      \"with n_clusters = %d\" % n_clusters),\n",
        "                     fontsize=14, fontweight='bold')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    style.use(\"fivethirtyeight\")\n",
        "    plt.plot(range_n_clusters, silhouette_avg_n_clusters)\n",
        "    plt.xlabel(\"Number of Clusters (k)\")\n",
        "    plt.ylabel(\"Silhouette Score\")\n",
        "    plt.show()\n",
        "#KMEANS++\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "from matplotlib import cm\n",
        "\n",
        "def perform_rfm_analysis_updated(X):\n",
        "    range_n_clusters = list(range(2, 11))  # Update the range from 2 to 10\n",
        "\n",
        "    silhouette_scores = []\n",
        "    num_clusters = []\n",
        "\n",
        "    for n_clusters in range_n_clusters:\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "        fig.set_size_inches(18, 7)\n",
        "\n",
        "        ax1.set_xlim([-0.1, 1])\n",
        "        ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\n",
        "        clusterer = KMeans(n_clusters=n_clusters, random_state=42, init='k-means++')\n",
        "        cluster_labels = clusterer.fit_predict(X)\n",
        "\n",
        "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "        print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is:\", silhouette_avg)\n",
        "\n",
        "        silhouette_scores.append(silhouette_avg)\n",
        "        num_clusters.append(n_clusters)\n",
        "\n",
        "        sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "        y_lower = 10\n",
        "        for i in range(n_clusters):\n",
        "            ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
        "            ith_cluster_silhouette_values.sort()\n",
        "\n",
        "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "            y_upper = y_lower + size_cluster_i\n",
        "\n",
        "            color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "            ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values,\n",
        "                              facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "            y_lower = y_upper + 10\n",
        "\n",
        "        ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "        ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "        ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "        ax1.set_yticks([])\n",
        "        ax1.set_xticks([-0.2, -0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "        colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
        "        ax2.scatter(X.iloc[:, 0], X.iloc[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
        "                    c=colors, edgecolor='k')\n",
        "\n",
        "        centers = clusterer.cluster_centers_\n",
        "        ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
        "                    c=\"white\", alpha=1, s=200, edgecolor='k')\n",
        "\n",
        "        for i, c in enumerate(centers):\n",
        "            ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
        "                        s=50, edgecolor='k')\n",
        "\n",
        "        ax2.set_title(\"The visualization of the clustered data.\")\n",
        "        ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "        ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "        plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                      \"with n_clusters = %d\" % n_clusters),\n",
        "                     fontsize=14, fontweight='bold')\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(num_clusters, silhouette_scores, 'bo-')\n",
        "    plt.xlabel(\"Number of clusters\")\n",
        "    plt.ylabel(\"Silhouette score\")\n",
        "    plt.title(\"Silhouette score vs. Number of clusters\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# DAVID BOULDIN INDEX\n",
        "#K MEANS\n",
        "\n",
        "def plot_davies_bouldin_index_kmeans(X, min_clusters, max_clusters):\n",
        "    results = {}\n",
        "\n",
        "    for i in range(min_clusters, max_clusters+1):\n",
        "        kmeans = KMeans(n_clusters=i, random_state=30)\n",
        "        labels = kmeans.fit_predict(X)\n",
        "        db_index = davies_bouldin_score(X, labels)\n",
        "        results[i] = db_index\n",
        "\n",
        "    plt.plot(list(results.keys()), list(results.values()))\n",
        "    plt.xlabel(\"Number of clusters\")\n",
        "    plt.ylabel(\"Davies-Bouldin Index\")\n",
        "    plt.title(\"Davies-Bouldin Index vs Number of Clusters\")\n",
        "    plt.show()\n",
        "\n",
        "#KMEANS++\n",
        "def plot_davies_bouldin_index_kmeansplusplus(X, min_clusters, max_clusters, init='k-means++'):\n",
        "    results = {}\n",
        "\n",
        "    for i in range(min_clusters, max_clusters+1):\n",
        "        kmeans = KMeans(n_clusters=i, init=init, random_state=30)\n",
        "        labels = kmeans.fit_predict(X)\n",
        "        db_index = davies_bouldin_score(X, labels)\n",
        "        results[i] = db_index\n",
        "\n",
        "    plt.plot(list(results.keys()), list(results.values()))\n",
        "    plt.xlabel(\"Number of clusters\")\n",
        "    plt.ylabel(\"Davies-Bouldin Index\")\n",
        "    plt.title(\"Davies-Bouldin Index vs Number of Clusters\")\n",
        "    plt.show()\n",
        "\n",
        "#K-MEDOIDS\n",
        "\n",
        "\n",
        "def perform_silhouette_analysis_kmedoids(data):\n",
        "    \"\"\"\n",
        "    Perform silhouette analysis for k-means clustering on RFM data.\n",
        "\n",
        "    Args:\n",
        "        data (numpy.ndarray or pandas.DataFrame): RFM data for clustering.\n",
        "\n",
        "    Returns:\n",
        "        None (plots the silhouette scores).\n",
        "\n",
        "    \"\"\"\n",
        "    # Define a range of k values\n",
        "    k_values = range(2, 11)\n",
        "\n",
        "    # Initialize lists to store silhouette scores\n",
        "    silhouette_scores = []\n",
        "\n",
        "    # Perform k-means clustering for each k value\n",
        "    for k in k_values:\n",
        "        # Fit k-means model\n",
        "        kmeans = KMeans(n_clusters=k)\n",
        "        kmeans.fit(data)\n",
        "\n",
        "        # Predict cluster labels\n",
        "        labels = kmeans.labels_\n",
        "\n",
        "        # Calculate silhouette score\n",
        "        score = silhouette_score(data, labels)\n",
        "\n",
        "        # Store the silhouette score\n",
        "        silhouette_scores.append(score)\n",
        "\n",
        "    # Plot the silhouette scores\n",
        "    plt.plot(k_values, silhouette_scores)\n",
        "    plt.xlabel('Number of clusters (k)')\n",
        "    plt.ylabel('Silhouette score')\n",
        "    plt.title('Silhouette Analysis')\n",
        "    plt.show()\n",
        "#K DBSCAN\n",
        "\n",
        "\n",
        "\n",
        "def plot_davies_bouldin_index_som(X, min_clusters, max_clusters):\n",
        "    results = {}\n",
        "\n",
        "    for i in range(min_clusters, max_clusters+1):\n",
        "        som = MiniSom(i, 1, X.shape[1], sigma=0.5, learning_rate=0.5)\n",
        "        som.train_batch(X, num_iteration=100)\n",
        "        labels = np.array([som.winner(x)[0] for x in X])\n",
        "        db_index = davies_bouldin_score(X, labels)\n",
        "        results[i] = db_index\n",
        "\n",
        "    plt.plot(list(results.keys()), list(results.values()))\n",
        "    plt.xlabel(\"Number of clusters\")\n",
        "    plt.ylabel(\"Davies-Bouldin Index\")\n",
        "    plt.title(\"Davies-Bouldin Index vs Number of Clusters\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zb3d_fINkMz9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KikCywT2blzi"
      },
      "outputs": [],
      "source": [
        "selected_columns_kmeans=RFM_log_trans_wo_scaled[['Recency','Frequency','Monetary']]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-Organizing Maps method (SOM)"
      ],
      "metadata": {
        "id": "LcUTfODVtQNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the range of cluster counts\n",
        "min_clusters = 2\n",
        "max_clusters = 12\n",
        "\n",
        "# Call the function and plot the Davies-Bouldin Index\n",
        "plot_davies_bouldin_index_som(selected_columns_kmeans.values, min_clusters, max_clusters)\n"
      ],
      "metadata": {
        "id": "7U_iwFLii7lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s9rWjxkWbhYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DBSCAN"
      ],
      "metadata": {
        "id": "Xzxd47yutgFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sBGlXwOVtgBM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ED80jxvGtfOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-MEANS"
      ],
      "metadata": {
        "id": "3j5GlXbt1qxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the range of cluster counts\n",
        "min_clusters = 2\n",
        "max_clusters = 10\n",
        "\n",
        "# Call the function and plot the Davies-Bouldin Index\n",
        "plot_davies_bouldin_index_kmeans(selected_columns_kmeans, min_clusters, max_clusters)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tl1Rbur71txQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Elbow Analysis\n",
        "k_values = [2, 3, 4, 5, 6, 8, 9, 10]\n",
        "elbow_method_kmeans(selected_columns_kmeans, k_values)"
      ],
      "metadata": {
        "id": "FDHsHaxRXEDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K_MEDOIDS\n"
      ],
      "metadata": {
        "id": "q8lgq1zp4gP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_rows_kmedoid=RFM_df[['Recency','Frequency','Monetary']]"
      ],
      "metadata": {
        "id": "1Vc_VNYpW3QK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the RFM data\n",
        "scaler = StandardScaler()\n",
        "rfm_data_scaled = scaler.fit_transform(selected_rows_kmedoid)\n",
        "\n",
        "# Convert the scaled data back to a DataFrame\n",
        "rfm_data_scaled_df = pd.DataFrame(rfm_data_scaled, columns=selected_rows_kmedoid.columns)"
      ],
      "metadata": {
        "id": "KTw7wFtaV9YK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perform_silhouette_analysis_kmedoids(rfm_data_scaled_df)"
      ],
      "metadata": {
        "id": "4nOigzCUW9lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-MEANS++"
      ],
      "metadata": {
        "id": "1IAammdnrRQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the range of cluster counts\n",
        "min_clusters = 2\n",
        "max_clusters = 10\n",
        "\n",
        "# Specify the initialization type\n",
        "init_type = 'k-means++'\n",
        "\n",
        "# Call the function and plot the Davies-Bouldin Index\n",
        "plot_davies_bouldin_index_kmeansplusplus(selected_columns_kmeans, min_clusters, max_clusters, init=init_type)\n",
        "\n"
      ],
      "metadata": {
        "id": "pfb6KdHK0ca8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EV4b7qhKAIfU"
      },
      "outputs": [],
      "source": [
        "#Elbow Analysis\n",
        "k_values = [2, 3, 4, 5, 6, 8, 9, 10]\n",
        "elbow_method_kmeans_plusplus(selected_columns_kmeans, k_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WixYy5pKtLxv"
      },
      "outputs": [],
      "source": [
        "perform_rfm_analysis_updated(selected_columns_kmeans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVyrwoM1-sfK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Assuming RFM_log_trans_wo_scaled is your DataFrame\n",
        "# Assuming num_clusters is the desired number of clusters\n",
        "\n",
        "# Perform k-means clustering with K-means++ initialization\n",
        "kmeans = KMeans(n_clusters=4, init='k-means++')\n",
        "kmeans.fit(RFM_log_trans_wo_scaled[['Recency','Frequency','Monetary']])\n",
        "\n",
        "# Get the cluster labels and cluster centers\n",
        "cluster_labels = kmeans.labels_\n",
        "cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "# Create a 3D plot\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Scatter plot the data points with color-coded clusters\n",
        "scatter = ax.scatter(RFM_log_trans_wo_scaled['Monetary'], RFM_log_trans_wo_scaled['Recency'], RFM_log_trans_wo_scaled['Frequency'], c=cluster_labels, cmap='viridis')\n",
        "\n",
        "# Scatter plot the cluster centers\n",
        "ax.scatter(cluster_centers[:, 2], cluster_centers[:, 0], cluster_centers[:, 1], c='red', marker='x', s=100)\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel('Monetary')\n",
        "ax.set_ylabel('Recency')\n",
        "ax.set_zlabel('Frequency')\n",
        "ax.set_title('K-means Clustering Results')\n",
        "\n",
        "# Set the viewing angle\n",
        "ax.view_init(azim=30, elev=10)  # Adjust the azim and elev angles to change the viewing angle\n",
        "\n",
        "# Create a colorbar legend\n",
        "plt.colorbar(scatter)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "# Add the cluster_labels as a new column in the DataFrame\n",
        "RFM_log_trans_wo_scaled['Cluster'] = cluster_labels\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(RFM_log_trans_wo_scaled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VOB50zShm7O"
      },
      "outputs": [],
      "source": [
        "RFM_log_trans_wo_scaled = RFM_log_trans_wo_scaled.merge(RFM_df[['customer_unique_id', 'Recency', 'Frequency', 'Monetary','RFMClass']], on='customer_unique_id', how='left')\n",
        "RFM_log_trans_wo_scaled[['Recency', 'Frequency', 'Monetary']] = RFM_log_trans_wo_scaled[['Recency_y', 'Frequency_y', 'Monetary_y']]\n",
        "RFM_log_trans_wo_scaled.drop(['Recency_x', 'Frequency_x', 'Monetary_x', 'Recency_y', 'Frequency_y', 'Monetary_y'], axis=1, inplace=True)\n",
        "RFM_log_trans_wo_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvhLO2e2Q1ft"
      },
      "outputs": [],
      "source": [
        "outliers = outliers.merge(RFM_df[['customer_unique_id', 'Recency', 'Frequency', 'Monetary','RFMClass']], on='customer_unique_id', how='left')\n",
        "outliers[['Recency', 'Frequency', 'Monetary']] = outliers[['Recency_y', 'Frequency_y', 'Monetary_y']]\n",
        "outliers.drop(['Recency_x', 'Frequency_x', 'Monetary_x', 'Recency_y', 'Frequency_y', 'Monetary_y'], axis=1, inplace=True)\n",
        "outliers.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z2U-wipPHKm"
      },
      "outputs": [],
      "source": [
        "outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyQ38AMmeJmK"
      },
      "outputs": [],
      "source": [
        "concatenated_df = pd.concat([RFM_log_trans_wo_scaled, outliers], axis=0)\n",
        "concatenated_df.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXYaUNEEj5FT"
      },
      "outputs": [],
      "source": [
        "concatenated_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za13pE_BtIsQ"
      },
      "outputs": [],
      "source": [
        "concatenated_df['Cluster'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQcQGLRy0NRz"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Group by 'Cluster' and calculate the averages\n",
        "cluster_averages = concatenated_df.groupby('Cluster')['Recency', 'Frequency', 'Monetary'].mean()\n",
        "\n",
        "# Calculate the overall averages\n",
        "overall_averages = concatenated_df[['Recency', 'Frequency', 'Monetary']].mean()\n",
        "\n",
        "# Get the size of each cluster\n",
        "cluster_sizes = concatenated_df['Cluster'].value_counts().rename('Cluster Size')\n",
        "\n",
        "# Create a new DataFrame to display the results\n",
        "result_df = pd.concat([cluster_averages, cluster_sizes], axis=1)\n",
        "result_df.loc['Overall'] = pd.Series({**overall_averages.to_dict(), 'Cluster Size': len(concatenated_df)})\n",
        "\n",
        "# Display the result table\n",
        "print(result_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PPnLcqRkqZ-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Convert the 'Cluster' column to integer\n",
        "concatenated_df['Cluster'] = concatenated_df['Cluster'].astype(int)\n",
        "\n",
        "# Dictionary mapping cluster IDs to names\n",
        "cluster_names = {\n",
        "    0: 'Cluster C1',\n",
        "    1: 'Cluster C2',\n",
        "    2: 'Cluster C3',\n",
        "    3: 'Cluster C4',\n",
        "    5: 'Cluster C5',\n",
        "}\n",
        "\n",
        "# Group the DataFrame by Cluster and RFMClass and calculate the count\n",
        "grouped = concatenated_df.groupby(['Cluster', 'RFMClass']).size().reset_index(name='Count')\n",
        "\n",
        "# Calculate the total count for each cluster\n",
        "cluster_totals = grouped.groupby('Cluster')['Count'].sum().reset_index(name='Total')\n",
        "\n",
        "# Filter out RFMClass categories with less than 2% count within each cluster\n",
        "filtered_grouped = grouped.merge(cluster_totals, on='Cluster')\n",
        "filtered_grouped['Percentage'] = filtered_grouped['Count'] / filtered_grouped['Total']\n",
        "filtered_grouped.loc[filtered_grouped['Percentage'] < 0.03, 'RFMClass'] = 'Other'\n",
        "\n",
        "# Get unique clusters\n",
        "clusters = filtered_grouped['Cluster'].unique()\n",
        "\n",
        "# Create subplots for each cluster with larger size\n",
        "num_clusters = len(clusters)\n",
        "fig, axs = plt.subplots(num_clusters, 1, figsize=(8, 6*num_clusters))\n",
        "\n",
        "# Iterate over clusters and create a pie chart for each\n",
        "for i, cluster in enumerate(clusters):\n",
        "    # Filter the grouped data for the current cluster\n",
        "    cluster_data = filtered_grouped[filtered_grouped['Cluster'] == cluster]\n",
        "    rfm_classes = cluster_data['RFMClass']\n",
        "    counts = cluster_data['Count']\n",
        "\n",
        "    # Calculate the count for the \"Other\" category within the current cluster\n",
        "    other_count = counts[rfm_classes == 'Other'].sum()\n",
        "\n",
        "    # Append the \"Other\" count to the counts array\n",
        "    counts = counts[rfm_classes != 'Other'].append(pd.Series(other_count, index=['Other']))\n",
        "\n",
        "    # Append the \"Other\" label to the rfm_classes array\n",
        "    rfm_classes = rfm_classes[rfm_classes != 'Other'].append(pd.Series('Other'))\n",
        "\n",
        "    # Sort the counts and rfm_classes arrays in descending order\n",
        "    counts, rfm_classes = zip(*sorted(zip(counts, rfm_classes), reverse=True))\n",
        "\n",
        "    # Create the pie chart with larger size\n",
        "    axs[i].pie(counts, labels=rfm_classes, autopct='%1.1f%%', startangle=90)\n",
        "    axs[i].set_title(cluster_names.get(cluster, f'Cluster {cluster}'))\n",
        "\n",
        "# Adjust the layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oFhn-gbScTP"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Group by 'Cluster' and calculate the averages\n",
        "cluster_averages = concatenated_df.groupby('Cluster')['Recency', 'Frequency', 'Monetary'].mean()\n",
        "\n",
        "# Calculate the overall averages\n",
        "overall_averages = concatenated_df[['Recency', 'Frequency', 'Monetary']].mean()\n",
        "\n",
        "# Get the size of each cluster\n",
        "cluster_sizes = concatenated_df['Cluster'].value_counts().rename('Cluster Size')\n",
        "\n",
        "# Create a new DataFrame to display the results\n",
        "result_df = pd.concat([cluster_averages, cluster_sizes], axis=1)\n",
        "result_df.loc['Overall'] = pd.Series({**overall_averages.to_dict(), 'Cluster Size': len(concatenated_df)})\n",
        "\n",
        "# Display the result table\n",
        "print(result_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXUI_iLSdRFE"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Assuming concatenated_df is the DataFrame with \"Recency\", \"Frequency\", \"Monetary\", and \"Cluster\" columns\n",
        "\n",
        "# Create a larger figure\n",
        "fig = plt.figure(figsize=(12, 9))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Get unique cluster labels\n",
        "clusters = RFM_log_trans_wo_scaled['Cluster'].unique()\n",
        "\n",
        "# Assign a different color to each cluster\n",
        "colors = plt.cm.viridis(clusters / max(clusters))\n",
        "\n",
        "# Scatter plot the data points with color-coded clusters\n",
        "for cluster, color in zip(clusters, colors):\n",
        "    cluster_data = RFM_log_trans_wo_scaled[RFM_log_trans_wo_scaled['Cluster'] == cluster]\n",
        "    ax.scatter(cluster_data['Frequency'], cluster_data['Recency'], cluster_data['Monetary'], c=color, label=f'Cluster {cluster}', s=1)  # Adjust the value of s to make the dots smaller\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel('Frequency')\n",
        "ax.set_ylabel('Recency')\n",
        "ax.set_zlabel('Monetary')\n",
        "#ax.set_title('3D Plot of RFM Variables Grouped by Clusters')\n",
        "ax.legend()\n",
        "\n",
        "# Set the range of each axis\n",
        "ax.set_xlim3d(0, 8)  # Modify the range of the x-axis\n",
        "ax.set_ylim3d(0, 800)  # Modify the range of the y-axis\n",
        "ax.set_zlim3d(0, 1200)  # Modify the range of the z-axis\n",
        "\n",
        "# Set the viewing angle\n",
        "ax.view_init(azim=50, elev=10)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZ1MWFMZY0xc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Assuming concatenated_df is the DataFrame with \"Recency\", \"Frequency\", \"Monetary\", and \"Cluster\" columns\n",
        "\n",
        "# Create a larger figure\n",
        "fig = plt.figure(figsize=(14, 10))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Get unique cluster labels\n",
        "clusters = RFM_log_trans_wo_scaled['Cluster'].unique()\n",
        "\n",
        "# Assign a different color to each cluster\n",
        "colors = plt.cm.viridis(clusters / max(clusters))\n",
        "\n",
        "# Scatter plot the data points with color-coded clusters\n",
        "for cluster, color in zip(clusters, colors):\n",
        "    cluster_data = RFM_log_trans_wo_scaled[RFM_log_trans_wo_scaled['Cluster'] == cluster]\n",
        "    ax.scatter(cluster_data['Frequency'], cluster_data['Recency'], cluster_data['Monetary'], c=color, label=f'Cluster {cluster}', s=1)  # Adjust the value of s to make the dots smaller\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel('Frequency', labelpad=7)  # Add space between the x-axis label and the axis\n",
        "ax.set_ylabel('Recency', labelpad=7)  # Add space between the y-axis label and the axis\n",
        "ax.set_zlabel('Monetary', labelpad=4)  # Add space between the z-axis label and the axis\n",
        "#ax.set_title('3D Plot of RFM Variables Grouped by Clusters')\n",
        "ax.legend()\n",
        "\n",
        "# Set the range of each axis\n",
        "ax.set_xlim3d(8, 0)  # Modify the range of the x-axis\n",
        "ax.set_ylim3d(0, 800)  # Modify the range of the y-axis\n",
        "ax.set_zlim3d(0, 1200)  # Modify the range of the z-axis\n",
        "\n",
        "# Set the viewing angle\n",
        "ax.view_init(azim=120, elev=10)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IsBeUy2Yctu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming concatenated_df is the DataFrame with \"Recency\", \"Frequency\", \"Monetary\", and \"Cluster\" columns\n",
        "\n",
        "# Create a figure\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# Get unique cluster labels\n",
        "clusters = RFM_log_trans_wo_scaled['Cluster'].unique()\n",
        "\n",
        "# Assign a different color to each cluster\n",
        "colors = plt.cm.viridis(clusters / max(clusters))\n",
        "\n",
        "# Scatter plot the data points with color-coded clusters\n",
        "for cluster, color in zip(clusters, colors):\n",
        "    cluster_data = RFM_log_trans_wo_scaled[RFM_log_trans_wo_scaled['Cluster'] == cluster]\n",
        "    ax.scatter(cluster_data['Frequency'], cluster_data['Recency'], c=color, label=f'Cluster {cluster}', s=10)  # Adjust the value of s to change the dot size\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel('Frequency')\n",
        "ax.set_ylabel('Recency')\n",
        "ax.set_title('Recency vs. Monetary')\n",
        "\n",
        "# Set the range of each axis\n",
        "ax.set_xlim(0, 8)  # Modify the range of the x-axis\n",
        "ax.set_ylim(0, 800)  # Modify the range of the y-axis\n",
        "\n",
        "# Show the legend\n",
        "#ax.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "44"
      ],
      "metadata": {
        "id": "eBpS_b0N1rUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldjee-1e_CLF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define a range of k values\n",
        "k_values = range(2, 10)\n",
        "\n",
        "# Initialize an empty list to store the sum of squared distances\n",
        "ssd = []\n",
        "\n",
        "# Calculate the sum of squared distances for each value of k\n",
        "for k in k_values:\n",
        "    kmedoids = KMedoids(n_clusters=k, random_state=0).fit(RFM_log_trans_wo_scaled[['Recency','Frequency','Monetary']])\n",
        "    ssd.append(kmedoids.inertia_)\n",
        "\n",
        "# Plot the elbow curve\n",
        "plt.plot(k_values, ssd, marker='o')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Sum of Squared Distances')\n",
        "plt.title('Elbow Curve for K-Medoids Clustering')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KJaLKIV_CCS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9PfB4vA_B28"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ky4Bb2BCAkQo"
      },
      "outputs": [],
      "source": [
        "#Assing higher level categories for product categories\n",
        "def categorize_product(row):\n",
        "    subcategory = row['product_category_name_english']\n",
        "    if subcategory in ['bed_bath_table', 'furniture_decor', 'home_appliances', 'home_construction', 'home_confort',\n",
        "                       'kitchen_dining_laundry_garden_furniture', 'furniture_living_room', 'furniture_bedroom',\n",
        "                       'furniture_mattress_and_upholstery',  'pet_shop', 'flowers', 'home_comfort_2', 'la_cuisine', 'air_conditioning'  ]:\n",
        "        return 'Home and Furniture'\n",
        "    elif subcategory in ['health_beauty', 'diapers_and_hygiene', 'perfumery']:\n",
        "\n",
        "        return 'Health and Beauty'\n",
        "    elif subcategory in ['sports_leisure', 'toys', 'cool_stuff', 'musical_instruments', 'sports_leisure', 'christmas_supplies', 'party_supplies',\n",
        "                         'art', 'cine_photo', 'dvds_blu_ray', 'music','cds_dvds_musicals','arts_and_craftmanship']:\n",
        "        return 'Entertainment'\n",
        "    elif subcategory in ['computers_accessories', 'electronics', 'office_furniture', 'consoles_games',\n",
        "                         'tablets_printing_image', 'computers', 'telephony', 'fixed_telephony', 'audio','signaling_and_security',\n",
        "                         'security_and_services']:\n",
        "\n",
        "        return 'Technology and Office'\n",
        "    elif subcategory in ['housewares', 'home_appliances_2', 'small_appliances', 'small_appliances_home_oven_and_coffee']:\n",
        "\n",
        "        return 'Appliances and Housewares'\n",
        "    elif subcategory in ['watches_gifts','luggage_accessories', 'luggage_accessories ',\n",
        "                         'fashion_bags_accessories ', 'fashion_bags_accessories', 'fashion_shoes', 'fashio_female_clothing',\n",
        "                         'fashion_male_clothing', 'fashion_underwear_beach', 'fashion_childrens_clothes',\n",
        "                         'fashion_sport','baby']:\n",
        "        return 'Fashion and Accesories'\n",
        "    elif subcategory in ['garden_tools', 'construction_tools_construction', 'costruction_tools_garden', 'construction_tools_lights',\n",
        "                         'costruction_tools_tools', 'construction_tools_safety'\n",
        "                         ]:\n",
        "        return 'Garden and Construction'\n",
        "    elif subcategory in ['auto']:\n",
        "        return 'Automotive'\n",
        "    elif subcategory in ['food', 'drinks', 'food_drink']:\n",
        "        return 'Food and Beverages'\n",
        "    elif subcategory in ['stationery', 'books_general_interest', 'books_technical', 'books_imported']:\n",
        "        return 'Books and Stationery'\n",
        "    elif subcategory in ['industry_commerce_and_business', 'market_place','agro_industry_and_commerce']:\n",
        "        return 'Business and Industrial'\n",
        "    else:\n",
        "        return 'Other'\n",
        "\n",
        "olist_df['category'] = olist_df.apply(categorize_product, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nhm3TFfcInhy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GN_Dzj0jIpoJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtXU_MsuIoxa"
      },
      "source": [
        "# PRODUCT ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU3votEzJYRW"
      },
      "outputs": [],
      "source": [
        "# Extract unique values from 'product_category_name_english'\n",
        "unique_categories = olist_df_cleaned['product_category_name_english'].unique()\n",
        "\n",
        "# Create a new DataFrame to store the unique categories\n",
        "unique_categories_df = pd.DataFrame({'product_category_name_english': unique_categories})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMHlPTRyMemo"
      },
      "outputs": [],
      "source": [
        "from mlxtend.frequent_patterns import apriori\n",
        "from mlxtend.frequent_patterns import association_rules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Prepare your transactional data\n",
        "# Assume you have a DataFrame named 'data' with columns: 'Transaction_ID', 'Product_Category'\n",
        "# Each row represents a transaction\n",
        "# Assume you have another DataFrame named 'RFM_df' with columns: 'Customer_ID', 'Recency', 'Frequency', 'Monetary'\n",
        "\n",
        "# Step 2: Data Encoding\n",
        "# Convert the data into a binary format suitable for Apriori algorithm\n",
        "data_encoded = pd.get_dummies(olist_df_cleaned['product_category_name_english'])\n",
        "rfm_encoded = pd.get_dummies(RFM_df[['Recency', 'Frequency', 'Monetary']])\n",
        "\n",
        "# Merge the data based on the customer ID\n",
        "data_combined = data_encoded.merge(rfm_encoded, left_index=True, right_index=True)\n",
        "\n",
        "# Binarize the data\n",
        "data_combined[data_combined > 0] = 1\n",
        "\n",
        "# Step 3: Set Support and Confidence Thresholds\n",
        "support_threshold = 0.1\n",
        "confidence_threshold = 0.5\n",
        "\n",
        "# Step 4: Apply the Apriori algorithm\n",
        "frequent_itemsets = apriori(data_combined.astype(bool), min_support=support_threshold, use_colnames=True)\n",
        "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=confidence_threshold)\n",
        "\n",
        "# Step 5: Analyze the Results\n",
        "print(rules)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWtgQfrk3yCR"
      },
      "outputs": [],
      "source": [
        "from mlxtend.frequent_patterns import apriori\n",
        "from mlxtend.frequent_patterns import association_rules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEbrSU0x3x53"
      },
      "outputs": [],
      "source": [
        "olist_apriori = olist[data['Country']=='United Kingdom']\n",
        "data_apriori.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-meqLtUq3xlA"
      },
      "outputs": [],
      "source": [
        "# Which Product and Their Count\n",
        "data_apr = data_apriori.groupby(['InvoiceNo', 'Description'])['Quantity'].sum().unstack().reset_index().fillna(0).set_index('InvoiceNo')\n",
        "data_apr.head()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPgT6oQzu8VBSIv/4wb85IE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}